{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ae23b4-73ff-42fa-b9c7-0ac69b71e02d",
   "metadata": {},
   "source": [
    "# Homework 3 - CNN on Multilabel remote sensing images\n",
    "## By: Syed Ali Murad Tahir\n",
    "\n",
    "In this homework we will be performing CNN on UCMerced dataset. This dataset contains Multispectral images. \n",
    "\n",
    "\n",
    "The task is divided into following step\n",
    "\n",
    "1)  Prepairing data\n",
    "2) The data we get, we divide that data, into training, test and validation. We divide training data into training and test\n",
    "3) To pretrain the model in order to avoid overfitting, we can use pretrained data already provided to us. Or, we can perform data augmentation\n",
    "to pre train the data so we have more samples of data\n",
    "4) Now we perform Convolution. So we choose the number of layers, neuron in that layer and the activation function of that layer (Check if need to perform padding before,). For padding check the following link: https://www.youtube.com/watch?v=mTVf7BN7S8w&list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO&index=27\n",
    "5) Then we add Max pooling to take out dominant features\n",
    "6) We can add dropout\n",
    "7) Then we have to perform flatten the data\n",
    "8) We can add a desne layer (Check if we need it or not) -> These are actually the hidden layers we specify when performing forward propgation\n",
    "9) Then we train or data\n",
    "10) After that we check our results on test data to check for overfitting\n",
    "11) At the end we check validation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849277b8-81c2-4a68-8b4d-f8be2b5b5e12",
   "metadata": {},
   "source": [
    "## Understanding the dataset\n",
    "\n",
    "This is a 21 class land use image dataset meant for research purposes. However, for multilabel, we only have 17 labels\n",
    "\n",
    "There are 100 images for each of the following classes:\n",
    "    \n",
    "    \n",
    " 1) airplane\t\n",
    " 2) bare-soil\t\n",
    " 3) buildings\t\n",
    " 4) cars\t\n",
    " 5) chaparral\t\n",
    " 6) court\t\n",
    " 7) dock\t\n",
    " 8) field\t\n",
    " 9) grass\t\n",
    " 10) mobile-home\t\n",
    " 11) pavement\t\n",
    " 12) sand\t\n",
    " 13) sea\t\n",
    " 14) ship\t\n",
    " 15) tanks\t\n",
    " 16) trees\t\n",
    " 17) water\n",
    "\n",
    "Each image measures 256x256 pixels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc67344-a238-412f-8080-3c4d35354b82",
   "metadata": {},
   "source": [
    "## Imporint Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e94269a-5bca-4a57-8c42-5f7439f4773a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:08:11.702382Z",
     "iopub.status.busy": "2022-07-16T20:08:11.701882Z",
     "iopub.status.idle": "2022-07-16T20:08:14.830746Z",
     "shell.execute_reply": "2022-07-16T20:08:14.829665Z",
     "shell.execute_reply.started": "2022-07-16T20:08:11.702271Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from simple_downloader import download\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, f1_score, average_precision_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, Subset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d177c7b-b47a-40b1-9be2-4ba6c56240fe",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8c1fb-493d-4f50-982e-7fa7bcdf621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = Path(\"./Hw3_data\")\n",
    "download_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44799501-6f9f-48f3-972f-e06da44261b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUB_URL = \"https://tubcloud.tu-berlin.de/s/H4QHX5GPDY6wDog/download/UCMerced_LandUse.zip\"\n",
    "output_file = download(TUB_URL, \"./Hw3_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae871bb-0c4f-4ccb-9e22-7081116dd301",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipf = zipfile.ZipFile(output_file)\n",
    "zipf.extractall(path=\"Hw3_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51bc050-301b-4730-9dff-b0a59de81aa9",
   "metadata": {},
   "source": [
    "## Visualizing our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93d92a8-6d59-46b2-b706-c12aa21c341d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:08:57.606186Z",
     "iopub.status.busy": "2022-07-16T20:08:57.605787Z",
     "iopub.status.idle": "2022-07-16T20:08:58.184756Z",
     "shell.execute_reply": "2022-07-16T20:08:58.183838Z",
     "shell.execute_reply.started": "2022-07-16T20:08:57.606159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /notebooks/Hw3_data/UCMerced_LandUse/multilabels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMAGE\\LABEL</th>\n",
       "      <th>airplane</th>\n",
       "      <th>bare-soil</th>\n",
       "      <th>buildings</th>\n",
       "      <th>cars</th>\n",
       "      <th>chaparral</th>\n",
       "      <th>court</th>\n",
       "      <th>dock</th>\n",
       "      <th>field</th>\n",
       "      <th>grass</th>\n",
       "      <th>mobile-home</th>\n",
       "      <th>pavement</th>\n",
       "      <th>sand</th>\n",
       "      <th>sea</th>\n",
       "      <th>ship</th>\n",
       "      <th>tanks</th>\n",
       "      <th>trees</th>\n",
       "      <th>water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agricultural00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agricultural01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agricultural02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agricultural03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agricultural04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>tenniscourt95</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>tenniscourt96</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>tenniscourt97</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>tenniscourt98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>tenniscourt99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2100 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         IMAGE\\LABEL  airplane  bare-soil  buildings  cars  chaparral  court  \\\n",
       "0     agricultural00         0          0          0     0          0      0   \n",
       "1     agricultural01         0          0          0     0          0      0   \n",
       "2     agricultural02         0          0          0     0          0      0   \n",
       "3     agricultural03         0          0          0     0          0      0   \n",
       "4     agricultural04         0          0          0     0          0      0   \n",
       "...              ...       ...        ...        ...   ...        ...    ...   \n",
       "2095   tenniscourt95         0          1          0     0          0      1   \n",
       "2096   tenniscourt96         0          1          1     1          0      1   \n",
       "2097   tenniscourt97         0          1          1     1          0      1   \n",
       "2098   tenniscourt98         0          1          1     1          0      1   \n",
       "2099   tenniscourt99         0          1          0     1          0      1   \n",
       "\n",
       "      dock  field  grass  mobile-home  pavement  sand  sea  ship  tanks  \\\n",
       "0        0      1      0            0         0     0    0     0      0   \n",
       "1        0      1      0            0         0     0    0     0      0   \n",
       "2        0      1      0            0         0     0    0     0      0   \n",
       "3        0      1      0            0         0     0    0     0      0   \n",
       "4        0      0      0            0         0     0    0     0      0   \n",
       "...    ...    ...    ...          ...       ...   ...  ...   ...    ...   \n",
       "2095     0      0      1            0         1     0    0     0      0   \n",
       "2096     0      0      1            0         1     0    0     0      0   \n",
       "2097     0      0      1            0         1     0    0     0      0   \n",
       "2098     0      0      1            0         1     0    0     0      0   \n",
       "2099     0      0      1            0         1     0    0     0      0   \n",
       "\n",
       "      trees  water  \n",
       "0         1      0  \n",
       "1         0      0  \n",
       "2         0      0  \n",
       "3         0      0  \n",
       "4         1      0  \n",
       "...     ...    ...  \n",
       "2095      0      0  \n",
       "2096      1      0  \n",
       "2097      1      0  \n",
       "2098      1      0  \n",
       "2099      1      0  \n",
       "\n",
       "[2100 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Go to different file directory\n",
    "path = os.path.join(\"/notebooks\", \"Hw3_data\", \"UCMerced_LandUse\", \"multilabels\", \"\")\n",
    "os.chdir(path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))\n",
    "\n",
    "\n",
    "data = pd.DataFrame(pd.read_excel(\"LandUse_Multilabeled.xlsx\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a7b62f0-ffc1-49e6-a73c-2f9d269bd06e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:08:58.186799Z",
     "iopub.status.busy": "2022-07-16T20:08:58.186522Z",
     "iopub.status.idle": "2022-07-16T20:08:58.509044Z",
     "shell.execute_reply": "2022-07-16T20:08:58.508044Z",
     "shell.execute_reply.started": "2022-07-16T20:08:58.186758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Number of labels\n",
      "airplane                 100\n",
      "bare-soil                718\n",
      "buildings                691\n",
      "cars                     886\n",
      "chaparral                115\n",
      "court                    105\n",
      "dock                     100\n",
      "field                    103\n",
      "grass                    975\n",
      "mobile-home              102\n",
      "pavement                1300\n",
      "sand                     294\n",
      "sea                      100\n",
      "ship                     102\n",
      "tanks                    100\n",
      "trees                   1009\n",
      "water                    203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Total no. of labels that belong to each class')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = data.sum(axis=\"rows\")\n",
    "df = pd.DataFrame(result,columns=['Number of labels'])   #Convert into Data Frame\n",
    "#Drop the first row\n",
    "df.drop(index=df.index[0], \n",
    "        axis=0, \n",
    "        inplace=True)\n",
    "print(df)\n",
    "\n",
    "\n",
    "df.plot.bar(figsize=(10,10))\n",
    "plt.xlabel(\"Total Classes\")\n",
    "plt.ylabel(\"No. of Labels\")\n",
    "plt.title(\"Total no. of labels that belong to each class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b3e66-0f3f-4d5c-a749-3eaee0efec8f",
   "metadata": {},
   "source": [
    "## Designing Data Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b4a6bed-5835-4132-b310-a77f67f2793c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:09:00.779381Z",
     "iopub.status.busy": "2022-07-16T20:09:00.778648Z",
     "iopub.status.idle": "2022-07-16T20:09:00.790766Z",
     "shell.execute_reply": "2022-07-16T20:09:00.789449Z",
     "shell.execute_reply.started": "2022-07-16T20:09:00.779352Z"
    }
   },
   "outputs": [],
   "source": [
    "class UCMerced(Dataset):\n",
    "    def __init__(self, root_dir, img_transform=None, multilabel=True):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.images_path = os.path.join(root_dir, \"Images\")\n",
    "        self.class_names = sorted(\n",
    "            [cl for cl in os.listdir(self.images_path) if not cl.startswith(\".\")]\n",
    "        )\n",
    "        self.img_paths, self.img_labels = self.init_dataset()\n",
    "        self.img_transform = img_transform\n",
    "\n",
    "        if multilabel:\n",
    "            self.img_labels = self.read_multilabels()  # important for loss calculation\n",
    "            self.img_labels = self.img_labels.astype(float)\n",
    "\n",
    "    def init_dataset(self):\n",
    "        img_paths, img_labels = [], []\n",
    "        for cl_id, cl_name in enumerate(self.class_names):\n",
    "            cl_path = os.path.join(self.images_path, cl_name)\n",
    "\n",
    "            for img in sorted(os.listdir(cl_path)):\n",
    "                img_path = os.path.join(cl_path, img)\n",
    "                img_paths.append(img_path)\n",
    "                img_labels.append(cl_id)\n",
    "\n",
    "        return img_paths, img_labels\n",
    "\n",
    "    def read_multilabels(self):\n",
    "        label = pd.read_excel(\"./Hw3_data/UCMerced_LandUse/multilabels/LandUse_Multilabeled.xlsx\")\n",
    "        label = label.set_index(\"IMAGE\\LABEL\")\n",
    "        label = np.array(label)\n",
    "        return label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.img_transform is not None:\n",
    "            img = self.img_transform(img)\n",
    "\n",
    "        return dict(img=img, label=label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeff5703-931e-45ca-b2a0-7e36913630dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:09:01.786190Z",
     "iopub.status.busy": "2022-07-16T20:09:01.785087Z",
     "iopub.status.idle": "2022-07-16T20:09:01.793720Z",
     "shell.execute_reply": "2022-07-16T20:09:01.792328Z",
     "shell.execute_reply.started": "2022-07-16T20:09:01.786146Z"
    }
   },
   "outputs": [],
   "source": [
    "class MetricTracker(object):\n",
    "    \"\"\"Computes and stores the average and current value.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af72c425-858b-4ac9-a29b-b134e9c8c97b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:09:03.354385Z",
     "iopub.status.busy": "2022-07-16T20:09:03.353433Z",
     "iopub.status.idle": "2022-07-16T20:09:03.359460Z",
     "shell.execute_reply": "2022-07-16T20:09:03.358595Z",
     "shell.execute_reply.started": "2022-07-16T20:09:03.354354Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_device(cuda_int):\n",
    "    \"\"\"Get Cuda-Device. If cuda_int < 0 compute on CPU.\"\"\"\n",
    "    if cuda_int < 0:\n",
    "        print(\"Computation on CPU\")\n",
    "        device = torch.device(\"cpu\")\n",
    "    elif torch.cuda.is_available():\n",
    "        print(\"Computation on CUDA GPU device {}\".format(cuda_int))\n",
    "        device = torch.device(\"cuda:{}\".format(cuda_int))\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a625c35c-4cff-429b-a3f4-db90a828456e",
   "metadata": {},
   "source": [
    "When obtaining our dataset, we divide our data randomly into training, testing and validation. We use random_split to perform this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cba7d50f-1d3e-4f4e-9a27-7adda55aa865",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:10:06.022457Z",
     "iopub.status.busy": "2022-07-16T20:10:06.021925Z",
     "iopub.status.idle": "2022-07-16T20:10:06.035119Z",
     "shell.execute_reply": "2022-07-16T20:10:06.033808Z",
     "shell.execute_reply.started": "2022-07-16T20:10:06.022417Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets ,models , transforms\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader ,random_split\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch.optim import Adam\n",
    "\n",
    "def get_dataset(root_dir, tr_transform,seed=1, multilabel=True):\n",
    "    valid_no = int(2100*0.1)\n",
    "    train_no = int(2100*0.7) \n",
    "    test_no = int(2100*0.2)\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameter\n",
    "    ---------\n",
    "    root_dir     : path to UCMerced Dataset\n",
    "    tr_transform : transformation for training data\n",
    "    te_transform : transformation for training data\n",
    "    set_sizes    : list of percentage of either train-test or train-val-test (sum to 100)\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    sets for train and test, optionally also val if len(set_sizes)==3\n",
    "    \"\"\"\n",
    "    ucm_dataset_tr = UCMerced(root_dir, img_transform=tr_transform, multilabel=multilabel)\n",
    "    #ucm_dataset_te = UCMerced(root_dir, img_transform=te_transform, multilabel=multilabel)\n",
    "    \n",
    "    trainset ,validset, testset  = random_split(ucm_dataset_tr, [train_no, valid_no, test_no])\n",
    "    \n",
    "    return trainset ,validset, testset\n",
    "    \n",
    "    \n",
    "    #idx_list = split_ucm_indices(set_sizes, seed=seed)\n",
    "\n",
    "   # train_set = Subset(ucm_dataset_tr, idx_list[0])\n",
    "    #test_set = Subset(ucm_dataset_te, idx_list[-1])\n",
    "\n",
    "    #if len(idx_list) > 2:\n",
    "     #   val_set = Subset(ucm_dataset_te, idx_list[1])\n",
    "      #  return train_set, val_set, test_set\n",
    "    #else:\n",
    "     #   return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa9e27cd-697e-494a-b93f-0f3ec05aaec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:10:06.613298Z",
     "iopub.status.busy": "2022-07-16T20:10:06.612862Z",
     "iopub.status.idle": "2022-07-16T20:10:06.622391Z",
     "shell.execute_reply": "2022-07-16T20:10:06.621195Z",
     "shell.execute_reply.started": "2022-07-16T20:10:06.613263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create class labels\n",
    "classLabels = [\"airplane\", \"bare-soil\", \"buildings\", \"cars\",\"chaparral\", \"court\", \"dock\", \"field\", \"grass\", \"mobile-home\",\"pavement\",\"sand\", \"sea\",\"ship\",\n",
    "               \"tanks\", \"trees\", \"water\"]\n",
    "\n",
    "import os\n",
    "os.chdir(\"/notebooks\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfe22e1-0d8c-415d-a0f4-c84b1c8ff3e0",
   "metadata": {},
   "source": [
    "# Training Design\n",
    "\n",
    "In this part, we will be creating a CNN. In order to perform CNN, we had to perform the following tasks: \n",
    "1) Divide our data into training, test and validation. We use 70% data for training, 20% for testing and 10% for validation. \n",
    "2) We create the Convolution layer. In this case we use the pytorch pretrained resnet 18 model \n",
    "3) For criterion, we use a BCEWithLogitsLoss function. This is due to the fact that we want to perform multilabel classification \n",
    "4) For optimizer, we use Adam optimizer. It is the same as stochastic gradient descent \n",
    "5) We create a dense layer that used for training the model \n",
    "6) We evaluate the model in order to check whether the model is overfitting or underfitting \n",
    "7) Test the model for unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb00ebf-e8d4-40b4-8a64-f9747e191565",
   "metadata": {},
   "source": [
    "\n",
    "In the first part, we divide our dataset into training test and validation. We provide 70% data to training, 10% to validation and 20% to testing. We already splitted the data randomly in the function get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06bcca3a-c413-46fc-ba33-bd03930c0e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:10:09.923547Z",
     "iopub.status.busy": "2022-07-16T20:10:09.923174Z",
     "iopub.status.idle": "2022-07-16T20:10:09.928874Z",
     "shell.execute_reply": "2022-07-16T20:10:09.927446Z",
     "shell.execute_reply.started": "2022-07-16T20:10:09.923518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation on CPU\n"
     ]
    }
   ],
   "source": [
    "#Choose our device\n",
    "cuda_device = get_device(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19551f-f162-4566-964c-49ab5627e29a",
   "metadata": {},
   "source": [
    "Now we need to choose our parameter. Since we only have 2100 images, it is recommended to use a smaller batch size so the gradient descent makes a small change after each iteration.\n",
    "We keep the learning rate small so we can see how our gradient descent reaches the global optima. We don't want want our gradient descent to get stuck at local optima\n",
    "Epoch is once all images are processed one time individually of forward and backward to the network. We try to keep Epoch between 20 - 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f2f9ad-276e-4d8c-a4a3-8f9a2d0809cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:14:06.533992Z",
     "iopub.status.busy": "2022-07-16T20:14:06.533652Z",
     "iopub.status.idle": "2022-07-16T20:14:06.538443Z",
     "shell.execute_reply": "2022-07-16T20:14:06.537152Z",
     "shell.execute_reply.started": "2022-07-16T20:14:06.533965Z"
    }
   },
   "outputs": [],
   "source": [
    "#Choose our parameter\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb0869-9a23-40da-b947-22d1eb5345a9",
   "metadata": {},
   "source": [
    "#Now we resize the images so all image have same size. his is how we transform all images. We do it for both training and testing data\n",
    "#For further understanding, pls refer to the following webste: https://www.programcreek.com/python/example/104832/torchvision.transforms.Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b104c9d-3e74-474d-ac16-f74251462c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:14:56.563168Z",
     "iopub.status.busy": "2022-07-16T20:14:56.562811Z",
     "iopub.status.idle": "2022-07-16T20:14:56.570325Z",
     "shell.execute_reply": "2022-07-16T20:14:56.568761Z",
     "shell.execute_reply.started": "2022-07-16T20:14:56.563141Z"
    }
   },
   "outputs": [],
   "source": [
    "ucm_mean = [0.595425, 0.3518577, 0.3225522]\n",
    "ucm_std = [0.19303136, 0.12492529, 0.10577361]\n",
    "\n",
    "\n",
    "tr_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=ucm_mean, std=ucm_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "te_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=ucm_mean, std=ucm_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f715ba-d293-46a8-a470-2954b3cbcac3",
   "metadata": {},
   "source": [
    "Divide our data into training, testing and validation and upload them into the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d0bb8c9-dee6-4579-9459-023ac17f47e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:15:36.312098Z",
     "iopub.status.busy": "2022-07-16T20:15:36.311737Z",
     "iopub.status.idle": "2022-07-16T20:15:37.110440Z",
     "shell.execute_reply": "2022-07-16T20:15:37.109331Z",
     "shell.execute_reply.started": "2022-07-16T20:15:36.312071Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "trainset, valset, testset = get_dataset(\n",
    "    \"./Hw3_data/UCMerced_LandUse\",\n",
    "    tr_transform=tr_transform,\n",
    "    multilabel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b6638c1-aa85-4824-b54e-8ce639983b55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:15:39.131744Z",
     "iopub.status.busy": "2022-07-16T20:15:39.131381Z",
     "iopub.status.idle": "2022-07-16T20:15:39.138150Z",
     "shell.execute_reply": "2022-07-16T20:15:39.137055Z",
     "shell.execute_reply.started": "2022-07-16T20:15:39.131718Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26768bb9-53c5-4c31-9c5a-b3708a76ecb5",
   "metadata": {},
   "source": [
    "## Define our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ea52a99-0770-43f6-b608-906da0b58ac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:15:50.227452Z",
     "iopub.status.busy": "2022-07-16T20:15:50.226915Z",
     "iopub.status.idle": "2022-07-16T20:15:57.706128Z",
     "shell.execute_reply": "2022-07-16T20:15:57.705234Z",
     "shell.execute_reply.started": "2022-07-16T20:15:50.227412Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/jovyan/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b4e87c235d41859a43d790c5915641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(512, 17) #21 - number of classes\n",
    "model.to(cuda_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77159ed1-9352-4ab3-b567-957f802b93a2",
   "metadata": {},
   "source": [
    "## Define our criterion and gradient descent as stochastic gradient \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54eb38b1-ee40-40b2-afc7-2aeaf4e8be0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:17:06.123594Z",
     "iopub.status.busy": "2022-07-16T20:17:06.123132Z",
     "iopub.status.idle": "2022-07-16T20:17:06.131869Z",
     "shell.execute_reply": "2022-07-16T20:17:06.130122Z",
     "shell.execute_reply.started": "2022-07-16T20:17:06.123565Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "#Criterion\n",
    "criterion = nn.BCEWithLogitsLoss().to(cuda_device)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd695f-a16d-4ef1-a1c9-11f11ed56960",
   "metadata": {},
   "source": [
    "## Define functions for training and validation\n",
    "\n",
    "In this section, we have to perform training and validation. During validation, we extract the probabilities for each class. Since we obtain integer values so we normalize these values between 0 & 1. Then we threshold the values hence values > 0.5 -> 1 && values < 0.5 -> 0.\n",
    "For detail understanding, please refer to the following link: https://discuss.pytorch.org/t/how-to-extract-probabilities/2720/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c87e91bb-b4d5-40a2-934e-ed33fee12c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:17:31.415570Z",
     "iopub.status.busy": "2022-07-16T20:17:31.415163Z",
     "iopub.status.idle": "2022-07-16T20:17:31.421582Z",
     "shell.execute_reply": "2022-07-16T20:17:31.420303Z",
     "shell.execute_reply.started": "2022-07-16T20:17:31.415542Z"
    }
   },
   "outputs": [],
   "source": [
    "#Normalization of array between 0 & 1\n",
    "\n",
    "def normalization(arr: np.array) -> np.array:\n",
    "    norm = []\n",
    "    min_val = np.amin(arr)\n",
    "    max_val = np.amax(arr)\n",
    "    \n",
    "    for i in arr:\n",
    "        for j in i:\n",
    "            z = ((arr[j] - min_val)/ (max_val -min_val))\n",
    "        norm.append(z)\n",
    "\n",
    "    norm = np.array(norm)\n",
    "    \n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee94cf8a-ef89-47e2-a13e-fee0a4810b1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:57:27.749381Z",
     "iopub.status.busy": "2022-07-16T20:57:27.749030Z",
     "iopub.status.idle": "2022-07-16T20:57:27.754735Z",
     "shell.execute_reply": "2022-07-16T20:57:27.753667Z",
     "shell.execute_reply.started": "2022-07-16T20:57:27.749356Z"
    }
   },
   "outputs": [],
   "source": [
    "def pretty_classification_report_print(report, class_names):\n",
    "    N = len(class_names)\n",
    "    df = pd.DataFrame(report).round(decimals=2)\n",
    "    df = df.rename(columns=dict(zip(list(map(str, range(N))), testset.dataset.class_names))).T\n",
    "    df[[\"support\"]] = df[[\"support\"]].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ffbff-4326-405f-9a7e-cbae54856808",
   "metadata": {},
   "source": [
    "In this section, we try to train our model:\n",
    "Since we want true probabilities to be equal to predicted probabilities, so we try to find probabilities for each class for all images. So we try to find the\n",
    "probability individually and thresholded by 0.5. This make sure that the values are divided into postive and negatives before calcualting the confusion matrix. \n",
    "\n",
    "For further understanding, the following website is extremely helpful: https://colab.research.google.com/github/kmkarakaya/ML_tutorials/blob/master/Multi_Label_Model_Evaulation.ipynb#scrollTo=u4iBlAQIQven\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b50b26-a3a0-4389-889f-c8c4f69d4c80",
   "metadata": {},
   "source": [
    "In our case, our microavg precision is acutally the accuracy of the model: \n",
    "\n",
    "To see, check the following websites: https://tomaxent.com/2018/04/27/Micro-and-Macro-average-of-Precision-Recall-and-F-Score/\n",
    "\n",
    "https://www.google.com/search?q=micro-average+of+precision+same+as+accuracy&sxsrf=ALiCzsZhA96dQoJ1g6cTEPqHavdM-U5XFQ%3A1657990742492&source=hp&ei=Vu7SYq3pGvaBxc8Pz_mLcA&iflsig=AJiK0e8AAAAAYtL8ZuAEt4IJKKTDrrOGNIPNmvfPvvGR&oq=Micro-average+of+precision+same+as+accuray&gs_lcp=Cgdnd3Mtd2l6EAMYADIHCCEQChCgATIHCCEQChCgATIHCCEQChCgATIHCCEQChCgAToGCAAQHhAWOgUIABCGAzoFCCEQoAE6CAghEB4QFhAdOgQIIRAVUABY1Rpg1CNoAHAAeACAAYMBiAGMDZIBBDEwLjeYAQCgAQKgAQE&sclient=gws-wiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a8d3a7b2-bf97-4df9-ac84-be19841ff20d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T21:01:36.362612Z",
     "iopub.status.busy": "2022-07-16T21:01:36.362207Z",
     "iopub.status.idle": "2022-07-16T21:01:36.370629Z",
     "shell.execute_reply": "2022-07-16T21:01:36.369515Z",
     "shell.execute_reply.started": "2022-07-16T21:01:36.362583Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, epochs, device, early_stop=False):\n",
    "    train_losses, val_losses = [], []\n",
    "    accuracy_scores = []\n",
    "    average_precision_score = []\n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_acc = 0\n",
    "    best_epoch = 1\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        print(\"Epoch {}/{}\".format(epoch, epochs))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        #val_loss = loss_tracker.avg, report = classification report, conf_mat = confusion matrix, overall_acc = micro avg \n",
    "        val_loss, report, conf_mat, avg_prec = val_epoch(model, val_loader, criterion, device) \n",
    "        overall_acc = report[\"micro avg\"][\"precision\"]\n",
    "        average_precision = avg_prec\n",
    "        \n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        accuracy_scores.append(overall_acc)\n",
    "        average_precision_score.append(average_precision)\n",
    "\n",
    "        \n",
    "        if best_acc < overall_acc:\n",
    "            best_acc = overall_acc\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        if epoch - best_epoch > 10 and early_stop:\n",
    "            break\n",
    "\n",
    "    return best_model, train_losses, val_losses, accuracy_scores, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "472ff0aa-cc35-41f4-8584-a94042d12026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T21:01:36.946889Z",
     "iopub.status.busy": "2022-07-16T21:01:36.946492Z",
     "iopub.status.idle": "2022-07-16T21:01:36.955394Z",
     "shell.execute_reply": "2022-07-16T21:01:36.954411Z",
     "shell.execute_reply.started": "2022-07-16T21:01:36.946860Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    loss_tracker = MetricTracker()\n",
    "    acc_tracker = MetricTracker()\n",
    "    model.train()\n",
    "\n",
    "    tqdm_bar = tqdm(train_loader, desc=\"Training: \")\n",
    "    for batch in tqdm_bar:\n",
    "\n",
    "        images = batch[\"img\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        batch_size = images.size(0)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        #loss = criterion(logits, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        loss_tracker.update(loss.item(), batch_size)\n",
    "\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        #batch_acc = (predicted == labels).sum().item() / batch_size\n",
    "        #acc_tracker.update(batch_acc, batch_size)\n",
    "        tqdm_bar.set_postfix(loss=loss_tracker.avg, accuracy=acc_tracker.avg)\n",
    "        tqdm_bar.set_postfix(loss=loss_tracker.avg)\n",
    "\n",
    "    return loss_tracker.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744453f-d94f-487e-83a9-ce06e18ba86b",
   "metadata": {},
   "source": [
    "1) Predict the probability of each class for a specfic image.\n",
    "2) Since all values are real number so we normalize all the values between 0 & 1\n",
    "3) Threshold values hence values > 0.5 -> 1 while values < 0.5 -> 0\n",
    "4) Implement the multilabel confusion matrix\n",
    "5) Design the classifcation report\n",
    "6) In order to obtain accuracy, we find the micro avg precision score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f3903f7-15aa-4e4a-9d2d-f8a9477debdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T21:01:37.865428Z",
     "iopub.status.busy": "2022-07-16T21:01:37.864995Z",
     "iopub.status.idle": "2022-07-16T21:01:37.878732Z",
     "shell.execute_reply": "2022-07-16T21:01:37.877264Z",
     "shell.execute_reply.started": "2022-07-16T21:01:37.865384Z"
    }
   },
   "outputs": [],
   "source": [
    "def val_epoch(model, val_loader, criterion, device):\n",
    "    loss_tracker = MetricTracker()\n",
    "    acc_tracker = MetricTracker()\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_prediction_normalize = []\n",
    "    y_pred_segmented = []\n",
    "    classLabels = [\"airplane\", \"bare-soil\", \"buildings\", \"cars\",\"chaparral\", \"court\", \"dock\", \"field\", \"grass\", \"mobile-home\",\"pavement\",\"sand\", \"sea\",\"ship\",\n",
    "               \"tanks\", \"trees\", \"water\"]\n",
    "    \n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        tqdm_bar = tqdm(val_loader, desc=\"Validation: \")\n",
    "        for batch in tqdm_bar:\n",
    "\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss_tracker.update(loss.item(), batch_size)\n",
    "\n",
    "            predicted_prob, predicted_labels = torch.topk(probs.data, 17) #Find values fo all classes\n",
    "           # batch_acc = (predicted == labels).sum().item() / batch_size\n",
    "            #acc_tracker.update(batch_acc, batch_size)\n",
    "            acc_tracker.update(batch_size)\n",
    "\n",
    "            y_pred += predicted_labels.tolist()\n",
    "            y_true += labels.tolist()\n",
    "            tqdm_bar.set_postfix(loss=loss_tracker.avg, accuracy=acc_tracker.avg)\n",
    "            \n",
    "            y_pred_arr = np.array(y_pred)\n",
    "            y_true_arr = np.array(y_true)\n",
    "            \n",
    "            #Normalize the y_prediction values between 0 & 1\n",
    "            y_prediction_normalize  = normalization(y_pred_arr)\n",
    "           \n",
    "            \n",
    "            \n",
    "            # Segmentation: Values > 0.5 -> 1 while other -> 0\n",
    "            \n",
    "            y_pred_segmented = np.where(y_prediction_normalize > 0.5, 1, 0) \n",
    "            y_pred_segmented = np.array(y_pred_segmented)\n",
    "            \n",
    "    #print(y_pred_segmented)\n",
    "    #print(y_pred_segmented.shape)\n",
    "      \n",
    "     \n",
    "    #Classfication_report, Multilabel confusion matrix, average precision score\n",
    "    \n",
    "    report = classification_report(y_true, y_pred_segmented,target_names=classLabels, zero_division=0, output_dict=True)\n",
    "    conf_mat = multilabel_confusion_matrix(y_true, y_pred_segmented)\n",
    "    avg_prec = average_precision_score(y_true, y_pred_segmented, average='macro', pos_label=1)\n",
    "    \n",
    "    #Visualize our classifcation report\n",
    "    pretty_classification_report_print(report, class_names=classLabels)\n",
    "\n",
    "\n",
    "        \n",
    "    return loss_tracker.avg, report, conf_mat, avg_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e9570c1-5452-4638-bead-b77061f6d648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T21:01:38.665199Z",
     "iopub.status.busy": "2022-07-16T21:01:38.664119Z",
     "iopub.status.idle": "2022-07-16T21:01:38.670669Z",
     "shell.execute_reply": "2022-07-16T21:01:38.669296Z",
     "shell.execute_reply.started": "2022-07-16T21:01:38.665147Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e17a9ee0-48c6-43df-9f7c-391be9c4325c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T21:01:39.403971Z",
     "iopub.status.busy": "2022-07-16T21:01:39.403533Z",
     "iopub.status.idle": "2022-07-16T21:02:50.274869Z",
     "shell.execute_reply": "2022-07-16T21:02:50.273768Z",
     "shell.execute_reply.started": "2022-07-16T21:01:39.403943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d08ddfe56994f889049aad999ae283c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422ffd22d8f641429ee1418865386f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040d2dc969b84d0399d0fb8a7a4e93f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83625c32312b45ef9e6553815dcda041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043864e492e74bad9668e2dc07928503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9524930e84c842159166000452985e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model, train_losses, val_losses, accuracy_scores, avg_precision_score = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=3,\n",
    "    device=cuda_device,\n",
    ")\n",
    "eval_accuracies.append(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81ed83-2365-4d7e-9342-5d4e1e569b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_classification_report_print(report, class_names=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e5a60-0c8c-496a-b99a-10d4532ec1f5",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fae4c3b-fa64-4335-a8a6-ee99f5c688ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:33:35.508072Z",
     "iopub.status.busy": "2022-07-16T20:33:35.506982Z",
     "iopub.status.idle": "2022-07-16T20:33:35.518396Z",
     "shell.execute_reply": "2022-07-16T20:33:35.517585Z",
     "shell.execute_reply.started": "2022-07-16T20:33:35.508027Z"
    }
   },
   "outputs": [],
   "source": [
    "#Testing val_epoch to test results\n",
    "#Check the following link: https://discuss.pytorch.org/t/how-to-extract-probabilities/2720/9\n",
    "\n",
    "def val_epoch(model, val_loader, criterion, device):\n",
    "    loss_tracker = MetricTracker()\n",
    "    acc_tracker = MetricTracker()\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_prediction_normalize = []\n",
    "    y_pred_segmented=[]\n",
    "    classLabels = [\"airplane\", \"bare-soil\", \"buildings\", \"cars\",\"chaparral\", \"court\", \"dock\", \"field\", \"grass\", \"mobile-home\",\"pavement\",\"sand\", \"sea\",\"ship\",\n",
    "               \"tanks\", \"trees\", \"water\"]\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        tqdm_bar = tqdm(val_loader, desc=\"Validation: \")\n",
    "        for batch in tqdm_bar:\n",
    "\n",
    "            images = batch[\"img\"].to(device) #Array of images\n",
    "            labels = batch[\"label\"].to(device) #Array of labels\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            loss_tracker.update(loss.item(), batch_size)\n",
    "            predicted_prob, predicted_labels = torch.topk(probs.data, 17)\n",
    "            #predicted_prob, predicted_labels = torch.max(probs.data, 1)\n",
    "            \n",
    "            \n",
    "           # batch_acc = (predicted == labels).sum().item() / batch_size\n",
    "            #acc_tracker.update(batch_acc, batch_size)\n",
    "            acc_tracker.update(batch_size)\n",
    "\n",
    "            y_pred += predicted_labels.tolist()\n",
    "            y_true += labels.tolist()\n",
    "            \n",
    "            y_pred_arr = np.array(y_pred)\n",
    "            y_true_arr = np.array(y_true)\n",
    "           \n",
    "        \n",
    "        return y_pred_arr, y_true_arr\n",
    "          \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60658d9b-5726-489a-ae80-d9beb439b73d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:34:25.331087Z",
     "iopub.status.busy": "2022-07-16T20:34:25.330687Z",
     "iopub.status.idle": "2022-07-16T20:34:26.367515Z",
     "shell.execute_reply": "2022-07-16T20:34:26.366162Z",
     "shell.execute_reply.started": "2022-07-16T20:34:25.331058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ecc96fcd9b4b029c3401c1cda373f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y true values are:  [[0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "(210, 17)\n",
      "Y prediction values are:  [[ 6  4 12 ... 16 13 15]\n",
      " [ 4 14  2 ...  1 15  7]\n",
      " [ 2  4  7 ...  8 13  1]\n",
      " ...\n",
      " [ 4 11  8 ...  1 15  9]\n",
      " [11  4 14 ... 13  9 15]\n",
      " [12  2  6 ...  3  8  1]]\n",
      "(210, 17)\n",
      "Y prediction Probability values are:  tensor([[0.1604, 0.1382, 0.1311, 0.0996, 0.0878, 0.0812, 0.0539, 0.0461, 0.0452,\n",
      "         0.0448, 0.0424, 0.0236, 0.0154, 0.0081, 0.0081, 0.0074, 0.0066],\n",
      "        [0.1862, 0.1480, 0.1281, 0.0684, 0.0677, 0.0640, 0.0635, 0.0616, 0.0515,\n",
      "         0.0341, 0.0335, 0.0310, 0.0250, 0.0175, 0.0131, 0.0046, 0.0023],\n",
      "        [0.2584, 0.2277, 0.1122, 0.0798, 0.0635, 0.0466, 0.0354, 0.0347, 0.0282,\n",
      "         0.0217, 0.0203, 0.0176, 0.0157, 0.0116, 0.0107, 0.0091, 0.0068],\n",
      "        [0.1758, 0.1219, 0.0921, 0.0788, 0.0760, 0.0695, 0.0621, 0.0601, 0.0537,\n",
      "         0.0461, 0.0365, 0.0274, 0.0270, 0.0203, 0.0195, 0.0173, 0.0159],\n",
      "        [0.2482, 0.1286, 0.1265, 0.0999, 0.0857, 0.0708, 0.0602, 0.0396, 0.0316,\n",
      "         0.0256, 0.0238, 0.0138, 0.0109, 0.0104, 0.0092, 0.0077, 0.0075],\n",
      "        [0.5166, 0.1008, 0.0717, 0.0702, 0.0617, 0.0330, 0.0242, 0.0186, 0.0175,\n",
      "         0.0174, 0.0153, 0.0133, 0.0120, 0.0113, 0.0078, 0.0063, 0.0024],\n",
      "        [0.2746, 0.1460, 0.1183, 0.0955, 0.0718, 0.0647, 0.0472, 0.0415, 0.0228,\n",
      "         0.0220, 0.0205, 0.0164, 0.0146, 0.0140, 0.0138, 0.0102, 0.0059],\n",
      "        [0.2244, 0.1957, 0.1601, 0.0777, 0.0616, 0.0573, 0.0522, 0.0424, 0.0405,\n",
      "         0.0390, 0.0151, 0.0090, 0.0082, 0.0080, 0.0059, 0.0019, 0.0010],\n",
      "        [0.4957, 0.1414, 0.1042, 0.0547, 0.0517, 0.0506, 0.0280, 0.0209, 0.0175,\n",
      "         0.0123, 0.0070, 0.0059, 0.0026, 0.0024, 0.0022, 0.0017, 0.0012],\n",
      "        [0.1819, 0.1391, 0.1184, 0.1122, 0.1079, 0.0756, 0.0713, 0.0571, 0.0375,\n",
      "         0.0231, 0.0221, 0.0174, 0.0105, 0.0088, 0.0081, 0.0065, 0.0025],\n",
      "        [0.1548, 0.1362, 0.1270, 0.0984, 0.0911, 0.0814, 0.0750, 0.0680, 0.0600,\n",
      "         0.0307, 0.0262, 0.0156, 0.0139, 0.0087, 0.0069, 0.0049, 0.0015],\n",
      "        [0.2164, 0.1061, 0.1008, 0.0890, 0.0883, 0.0758, 0.0639, 0.0569, 0.0563,\n",
      "         0.0486, 0.0278, 0.0207, 0.0149, 0.0121, 0.0100, 0.0066, 0.0056],\n",
      "        [0.2079, 0.1316, 0.0943, 0.0911, 0.0856, 0.0808, 0.0567, 0.0472, 0.0373,\n",
      "         0.0305, 0.0243, 0.0238, 0.0223, 0.0210, 0.0200, 0.0134, 0.0122],\n",
      "        [0.5497, 0.1323, 0.0628, 0.0593, 0.0431, 0.0376, 0.0256, 0.0208, 0.0174,\n",
      "         0.0141, 0.0101, 0.0088, 0.0068, 0.0055, 0.0023, 0.0021, 0.0017],\n",
      "        [0.2218, 0.1825, 0.1485, 0.1111, 0.0844, 0.0722, 0.0630, 0.0419, 0.0279,\n",
      "         0.0117, 0.0097, 0.0087, 0.0054, 0.0033, 0.0032, 0.0032, 0.0016],\n",
      "        [0.4307, 0.1174, 0.0749, 0.0722, 0.0616, 0.0551, 0.0454, 0.0307, 0.0237,\n",
      "         0.0169, 0.0165, 0.0161, 0.0133, 0.0131, 0.0046, 0.0042, 0.0035],\n",
      "        [0.1238, 0.1214, 0.1098, 0.0979, 0.0873, 0.0823, 0.0812, 0.0779, 0.0446,\n",
      "         0.0387, 0.0296, 0.0287, 0.0232, 0.0216, 0.0147, 0.0115, 0.0059],\n",
      "        [0.1820, 0.1443, 0.1182, 0.1132, 0.0787, 0.0651, 0.0391, 0.0383, 0.0350,\n",
      "         0.0328, 0.0313, 0.0249, 0.0241, 0.0237, 0.0235, 0.0171, 0.0086]])\n",
      "torch.Size([18, 17])\n"
     ]
    }
   ],
   "source": [
    "#Idea taken from the following website: https://colab.research.google.com/github/kmkarakaya/ML_tutorials/blob/master/Multi_Label_Model_Evaulation.ipynb#scrollTo=H8WBPN1OxXmw\n",
    "# We have 210 values because our validation takes 10% of our data so we get 210 values out of 2100\n",
    "y_prediction, y_true = val_epoch(model, val_loader, criterion, cuda_device)\n",
    "print(\"Y true values are: \", y_true)\n",
    "print(y_true.shape)\n",
    "\n",
    "print(\"Y prediction values are: \", y_prediction)\n",
    "print(y_prediction.shape)\n",
    "\n",
    "print(\"Y prediction Probability values are: \", y_predicted_prob)\n",
    "print(y_predicted_prob.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18ab0785-e808-41e3-b600-ac6994cb7bff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:38:22.721524Z",
     "iopub.status.busy": "2022-07-16T20:38:22.720991Z",
     "iopub.status.idle": "2022-07-16T20:38:22.743612Z",
     "shell.execute_reply": "2022-07-16T20:38:22.742491Z",
     "shell.execute_reply.started": "2022-07-16T20:38:22.721479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75   0.5    0.25   ... 0.625  0.5625 0.9375]\n",
      " [0.125  0.75   0.5625 ... 0.4375 0.8125 0.625 ]\n",
      " [0.25   0.875  0.125  ... 0.0625 0.9375 0.4375]\n",
      " ...\n",
      " [0.125  0.25   0.375  ... 0.8125 0.3125 0.9375]\n",
      " [0.75   0.5    0.25   ... 0.625  0.5625 0.9375]\n",
      " [0.25   0.875  0.125  ... 0.0625 0.9375 0.4375]]\n",
      "(210, 17)\n"
     ]
    }
   ],
   "source": [
    "# We can normalize the y_prediction values between 0 & 1\n",
    "\n",
    "#Move normalization code above\n",
    "\n",
    "def normalization(arr: np.array) -> np.array:\n",
    "    norm = []\n",
    "    min_val = np.amin(arr)\n",
    "    max_val = np.amax(arr)\n",
    "    \n",
    "    for i in arr:\n",
    "        for j in i:\n",
    "            z = ((arr[j] - min_val)/ (max_val -min_val))\n",
    "        norm.append(z)\n",
    "\n",
    "    norm = np.array(norm)\n",
    "    \n",
    "    return norm\n",
    "\n",
    "\n",
    "y_prediction_normalize  = normalization(y_prediction)\n",
    "print(y_prediction_normalize)\n",
    "print(y_prediction_normalize.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b9a63bd-d47d-4518-919b-e86222335f1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:38:24.203121Z",
     "iopub.status.busy": "2022-07-16T20:38:24.202759Z",
     "iopub.status.idle": "2022-07-16T20:38:24.210296Z",
     "shell.execute_reply": "2022-07-16T20:38:24.209249Z",
     "shell.execute_reply.started": "2022-07-16T20:38:24.203092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 1 1 1]\n",
      " [0 1 1 ... 0 1 1]\n",
      " [0 1 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 1]\n",
      " [1 0 0 ... 1 1 1]\n",
      " [0 1 0 ... 0 1 0]]\n",
      "(210, 17)\n"
     ]
    }
   ],
   "source": [
    "# Now we perform segmentation. Values > 0.5 -> 1 while other -> 0\n",
    "\n",
    "\n",
    "y_pred_segmented=[]\n",
    "\n",
    "y_pred_segmented = np.where(y_prediction_normalize > 0.5, 1, 0) \n",
    "\n",
    "y_pred_segmented = np.array(y_pred_segmented)\n",
    "print(y_pred_segmented)\n",
    "print(y_pred_segmented.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4c0846d-2012-4902-8da4-b6738bca4417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T20:32:24.751843Z",
     "iopub.status.busy": "2022-07-16T20:32:24.750809Z",
     "iopub.status.idle": "2022-07-16T20:32:24.771131Z",
     "shell.execute_reply": "2022-07-16T20:32:24.770297Z",
     "shell.execute_reply.started": "2022-07-16T20:32:24.751815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.11      0.64      0.18        14\n",
      "   bare-soil       0.43      0.32      0.37        68\n",
      "   buildings       0.31      0.22      0.25        69\n",
      "        cars       0.40      0.80      0.53        89\n",
      "   chaparral       0.15      0.81      0.25        16\n",
      "       court       0.08      0.70      0.14        10\n",
      "        dock       0.02      0.07      0.03        15\n",
      "       field       0.06      0.33      0.11        12\n",
      "       grass       0.45      0.66      0.53        88\n",
      " mobile-home       0.10      0.57      0.17        14\n",
      "    pavement       0.64      0.74      0.69       130\n",
      "        sand       0.15      0.59      0.24        27\n",
      "         sea       0.01      0.12      0.03         8\n",
      "        ship       0.04      0.20      0.07        15\n",
      "       tanks       0.06      0.73      0.12        11\n",
      "       trees       0.42      0.56      0.48        99\n",
      "       water       0.11      0.80      0.19        20\n",
      "\n",
      "   micro avg       0.24      0.57      0.34       705\n",
      "   macro avg       0.21      0.52      0.26       705\n",
      "weighted avg       0.37      0.57      0.42       705\n",
      " samples avg       0.24      0.58      0.33       705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix:\n",
    "from sklearn.metrics import multilabel_confusion_matrix, f1_score, average_precision_score\n",
    "st = multilabel_confusion_matrix(y_true, y_pred_segmented)\n",
    "\n",
    "\n",
    "classLabels = [\"airplane\", \"bare-soil\", \"buildings\", \"cars\",\"chaparral\", \"court\", \"dock\", \"field\", \"grass\", \"mobile-home\",\"pavement\",\"sand\", \"sea\",\"ship\",\n",
    "               \"tanks\", \"trees\", \"water\"]\n",
    "\n",
    "#Classifcation report\n",
    "#Check the following link to understand all the column: https://medium.com/synthesio-engineering/precision-accuracy-and-f1-score-for-multi-label-classification-34ac6bdfb404\n",
    "M = classification_report(y_true, y_pred_segmented,target_names=classLabels)\n",
    "print(M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1acdb3c2-acc9-47ac-a295-e590848da635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T21:00:22.124891Z",
     "iopub.status.busy": "2022-07-16T21:00:22.124058Z",
     "iopub.status.idle": "2022-07-16T21:00:22.134858Z",
     "shell.execute_reply": "2022-07-16T21:00:22.134103Z",
     "shell.execute_reply.started": "2022-07-16T21:00:22.124824Z"
    }
   },
   "outputs": [],
   "source": [
    "def val_epoch(model, val_loader, criterion, device):\n",
    "    loss_tracker = MetricTracker()\n",
    "    acc_tracker = MetricTracker()\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_prediction_normalize = []\n",
    "    y_pred_segmented = []\n",
    "    classLabels = [\"airplane\", \"bare-soil\", \"buildings\", \"cars\",\"chaparral\", \"court\", \"dock\", \"field\", \"grass\", \"mobile-home\",\"pavement\",\"sand\", \"sea\",\"ship\",\n",
    "               \"tanks\", \"trees\", \"water\"]\n",
    "    \n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        tqdm_bar = tqdm(val_loader, desc=\"Validation: \")\n",
    "        for batch in tqdm_bar:\n",
    "\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss_tracker.update(loss.item(), batch_size)\n",
    "\n",
    "            #_, predicted = torch.max(probs.data, 1)\n",
    "            predicted_prob, predicted_labels = torch.topk(probs.data, 17) #Find values fo all classes\n",
    "           # batch_acc = (predicted == labels).sum().item() / batch_size\n",
    "            #acc_tracker.update(batch_acc, batch_size)\n",
    "            acc_tracker.update(batch_size)\n",
    "\n",
    "            y_pred += predicted_labels.tolist()\n",
    "            y_true += labels.tolist()\n",
    "            tqdm_bar.set_postfix(loss=loss_tracker.avg, accuracy=acc_tracker.avg)\n",
    "            \n",
    "            y_pred_arr = np.array(y_pred)\n",
    "            y_true_arr = np.array(y_true)\n",
    "            \n",
    "            # We can normalize the y_prediction values between 0 & 1\n",
    "\n",
    "            \n",
    "            y_prediction_normalize  = normalization(y_pred_arr)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            min_val = np.amin(y_pred_arr)\n",
    "            max_val = np.amax(y_pred_arr)\n",
    "            for i in y_pred_arr:\n",
    "                for j in i:\n",
    "                    z = ((y_pred_arr[j] - min_val)/ (max_val -min_val))\n",
    "                y_prediction_normalize.append(z)\n",
    "\n",
    "            y_prediction_normalize = np.array(y_prediction_normalize)\n",
    "            print(y_prediction_normalize)\n",
    "            print(y_prediction_normalize.shape)\n",
    "            \n",
    "           \"\"\"         \n",
    "            \n",
    "            \n",
    "            # Now we perform segmentation. Values > 0.5 -> 1 while other -> 0\n",
    "            \n",
    "            y_pred_segmented = np.where(y_prediction_normalize > 0.5, 1, 0) \n",
    "            y_pred_segmented = np.array(y_pred_segmented)\n",
    "            \n",
    "    #print(y_pred_segmented)\n",
    "    #print(y_pred_segmented.shape)\n",
    "    \n",
    "      \n",
    "     \n",
    "    #Classfication_report and Multilabel confusion matrix\n",
    "    \n",
    "    #report = classification_report(y_true, y_pred_segmented,target_names=classLabels)\n",
    "    report = classification_report(y_true, y_pred_segmented, zero_division=0, output_dict=True)\n",
    "    conf_mat = multilabel_confusion_matrix(y_true, y_pred_segmented)\n",
    "        \n",
    "    return loss_tracker.avg, report, conf_mat, y_true, y_pred_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "729a2b36-f078-46b7-8ed5-246f3a8677cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T21:00:23.818250Z",
     "iopub.status.busy": "2022-07-16T21:00:23.817746Z",
     "iopub.status.idle": "2022-07-16T21:00:24.978866Z",
     "shell.execute_reply": "2022-07-16T21:00:24.977439Z",
     "shell.execute_reply.started": "2022-07-16T21:00:23.818221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d593cbd3520447f69fa3c7d626a9d818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.0975609756097561, 'recall': 0.5714285714285714, 'f1-score': 0.16666666666666669, 'support': 14}, '1': {'precision': 0.09090909090909091, 'recall': 0.10294117647058823, 'f1-score': 0.09655172413793103, 'support': 68}, '2': {'precision': 0.4583333333333333, 'recall': 0.6376811594202898, 'f1-score': 0.5333333333333333, 'support': 69}, '3': {'precision': 0.4927536231884058, 'recall': 0.7640449438202247, 'f1-score': 0.5991189427312775, 'support': 89}, '4': {'precision': 0.22916666666666666, 'recall': 0.6875, 'f1-score': 0.34375, 'support': 16}, '5': {'precision': 0.013157894736842105, 'recall': 0.1, 'f1-score': 0.023255813953488372, 'support': 10}, '6': {'precision': 0.01, 'recall': 0.06666666666666667, 'f1-score': 0.017391304347826087, 'support': 15}, '7': {'precision': 0.01, 'recall': 0.08333333333333333, 'f1-score': 0.017857142857142856, 'support': 12}, '8': {'precision': 0.4523809523809524, 'recall': 0.4318181818181818, 'f1-score': 0.4418604651162791, 'support': 88}, '9': {'precision': 0.058823529411764705, 'recall': 0.35714285714285715, 'f1-score': 0.10101010101010101, 'support': 14}, '10': {'precision': 0.6171875, 'recall': 0.6076923076923076, 'f1-score': 0.6124031007751937, 'support': 130}, '11': {'precision': 0.1134020618556701, 'recall': 0.4074074074074074, 'f1-score': 0.1774193548387097, 'support': 27}, '12': {'precision': 0.022900763358778626, 'recall': 0.375, 'f1-score': 0.04316546762589928, 'support': 8}, '13': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 15}, '14': {'precision': 0.060810810810810814, 'recall': 0.8181818181818182, 'f1-score': 0.11320754716981132, 'support': 11}, '15': {'precision': 0.37254901960784315, 'recall': 0.1919191919191919, 'f1-score': 0.2533333333333333, 'support': 99}, '16': {'precision': 0.2222222222222222, 'recall': 0.8, 'f1-score': 0.3478260869565218, 'support': 20}, 'micro avg': {'precision': 0.19107142857142856, 'recall': 0.4553191489361702, 'f1-score': 0.2691823899371069, 'support': 705}, 'macro avg': {'precision': 0.19542108494659627, 'recall': 0.4119269185471434, 'f1-score': 0.22871472852079502, 'support': 705}, 'weighted avg': {'precision': 0.35915488825668035, 'recall': 0.4553191489361702, 'f1-score': 0.3738381083994812, 'support': 705}, 'samples avg': {'precision': 0.19107142857142856, 'recall': 0.44532879818594107, 'f1-score': 0.25780642637785495, 'support': 705}}\n"
     ]
    }
   ],
   "source": [
    "val_loss, report, conf_matrix, y_true, y_pred_segmented = val_epoch(model, val_loader, criterion, cuda_device)\n",
    "print(report)\n",
    "#If we want overall accuracy score\n",
    "#accuracy = accuracy_score(y_true, y_pred_segmented)\n",
    "#print(accuracy)\n",
    "\n",
    "\n",
    "#If we want accuracy score for each class\n",
    "#Taken from the following website: https://stackoverflow.com/questions/39770376/scikit-learn-get-accuracy-scores-for-each-class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "334e5b30-a747-4599-97f2-0a18cc13be33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-16T21:00:26.113568Z",
     "iopub.status.busy": "2022-07-16T21:00:26.113114Z",
     "iopub.status.idle": "2022-07-16T21:00:26.137023Z",
     "shell.execute_reply": "2022-07-16T21:00:26.135822Z",
     "shell.execute_reply.started": "2022-07-16T21:00:26.113512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agricultural</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.17</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airplane</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseballdiamond</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.53</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beach</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.60</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.34</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chaparral</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denseresidential</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freeway</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.44</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>golfcourse</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>harbor</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intersection</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mediumresidential</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.04</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobilehomepark</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overpass</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parkinglot</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.25</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>river</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.35</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro avg</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.27</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.23</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.37</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samples avg</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.26</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   precision  recall  f1-score  support\n",
       "agricultural            0.10    0.57      0.17       14\n",
       "airplane                0.09    0.10      0.10       68\n",
       "baseballdiamond         0.46    0.64      0.53       69\n",
       "beach                   0.49    0.76      0.60       89\n",
       "buildings               0.23    0.69      0.34       16\n",
       "chaparral               0.01    0.10      0.02       10\n",
       "denseresidential        0.01    0.07      0.02       15\n",
       "forest                  0.01    0.08      0.02       12\n",
       "freeway                 0.45    0.43      0.44       88\n",
       "golfcourse              0.06    0.36      0.10       14\n",
       "harbor                  0.62    0.61      0.61      130\n",
       "intersection            0.11    0.41      0.18       27\n",
       "mediumresidential       0.02    0.38      0.04        8\n",
       "mobilehomepark          0.00    0.00      0.00       15\n",
       "overpass                0.06    0.82      0.11       11\n",
       "parkinglot              0.37    0.19      0.25       99\n",
       "river                   0.22    0.80      0.35       20\n",
       "micro avg               0.19    0.46      0.27      705\n",
       "macro avg               0.20    0.41      0.23      705\n",
       "weighted avg            0.36    0.46      0.37      705\n",
       "samples avg             0.19    0.45      0.26      705"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretty_classification_report_print(report, class_names=classLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b4ebd-8f48-4d3d-a193-4e4c4f296663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851df786-d2c2-4756-a1f5-c7bf830d089f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d242938-fd6b-470a-a271-b4d631c42a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f4cfb-03c9-4420-ba69-4da9ea643312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929061bd-9125-4bf0-91df-1c096a333e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa0d4d-fc41-45f1-a3cc-ea835fd2f282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b05ad8-a6b8-4f3e-a553-bc850daf5156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171a4ef-ade1-4c2b-b9af-708655bde7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb60e36-f55b-4645-82c4-bd77bdbb95e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739bf598-3d58-4ec9-9779-be34b2804d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f775b0-5f6d-4442-9d27-b77c8ab89d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0489f1-9fe1-481b-a517-7f4fce4e7b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac7fb6-a7ac-4863-ad8d-fff6c9febca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0773d-1d0d-4127-bc79-526dbbb87b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff31dda-829d-46a4-afd7-dd66d353ab41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ip4rs",
   "language": "python",
   "name": "ip4rs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
